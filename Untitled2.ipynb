{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d208cb-8d4f-4f02-b840-59629a7918bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      " [[0.56496031 0.73607334 0.59274022 ... 0.31940835 0.4644783  0.28763054]\n",
      " [0.97027829 0.59423112 0.77117653 ... 0.81909783 0.17422815 0.55832177]\n",
      " [0.81641632 0.39593447 0.93674751 ... 0.19642156 0.33796344 0.30454901]\n",
      " ...\n",
      " [0.35616825 0.24477677 0.73196159 ... 0.81807558 0.07418167 0.86345524]\n",
      " [0.98693668 0.95669402 0.69009518 ... 0.74846349 0.99426307 0.16303189]\n",
      " [0.99524877 0.51538237 0.93194001 ... 0.1776084  0.40848212 0.81305817]]\n",
      "\n",
      "Actual Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "\n",
      "Predicted Output: \n",
      " [[0.21222122]\n",
      " [0.1517028 ]\n",
      " [0.13590728]\n",
      " [0.14019269]\n",
      " [0.17823145]\n",
      " [0.13279531]\n",
      " [0.13122908]\n",
      " [0.18847043]\n",
      " [0.23193538]\n",
      " [0.15389265]\n",
      " [0.14477208]\n",
      " [0.09842617]\n",
      " [0.12701703]\n",
      " [0.16208482]\n",
      " [0.10076828]\n",
      " [0.18859068]\n",
      " [0.1450191 ]\n",
      " [0.14384588]\n",
      " [0.15425507]\n",
      " [0.107831  ]\n",
      " [0.17265873]\n",
      " [0.10116504]\n",
      " [0.30705455]\n",
      " [0.15894634]\n",
      " [0.17913859]\n",
      " [0.11164936]\n",
      " [0.13732743]\n",
      " [0.12804668]\n",
      " [0.14965659]\n",
      " [0.20200485]\n",
      " [0.14017282]\n",
      " [0.15069048]\n",
      " [0.15648565]\n",
      " [0.36183259]\n",
      " [0.23033417]\n",
      " [0.12516085]\n",
      " [0.26036482]\n",
      " [0.10073117]\n",
      " [0.14302739]\n",
      " [0.16522811]\n",
      " [0.23603053]\n",
      " [0.20872644]\n",
      " [0.10432143]\n",
      " [0.13483736]\n",
      " [0.11143938]\n",
      " [0.11317768]\n",
      " [0.2020262 ]\n",
      " [0.18847461]\n",
      " [0.17313507]\n",
      " [0.16184741]\n",
      " [0.16915918]\n",
      " [0.18212522]\n",
      " [0.11910239]\n",
      " [0.1206147 ]\n",
      " [0.14639991]\n",
      " [0.2680309 ]\n",
      " [0.22961993]\n",
      " [0.10342331]\n",
      " [0.12005753]\n",
      " [0.11167408]\n",
      " [0.21381692]\n",
      " [0.1093658 ]\n",
      " [0.14065952]\n",
      " [0.12790622]\n",
      " [0.11596352]\n",
      " [0.15847395]\n",
      " [0.1092727 ]\n",
      " [0.15896934]\n",
      " [0.1215608 ]\n",
      " [0.20545622]\n",
      " [0.11007275]\n",
      " [0.12898155]\n",
      " [0.11965312]\n",
      " [0.11965318]\n",
      " [0.1694395 ]\n",
      " [0.15751124]\n",
      " [0.14294671]\n",
      " [0.12696789]\n",
      " [0.21961146]\n",
      " [0.11457141]\n",
      " [0.11539721]\n",
      " [0.20945226]\n",
      " [0.11965341]\n",
      " [0.12085759]\n",
      " [0.12066909]\n",
      " [0.30701935]\n",
      " [0.10245243]\n",
      " [0.10645409]\n",
      " [0.20721599]\n",
      " [0.14474347]\n",
      " [0.15760215]\n",
      " [0.16004759]\n",
      " [0.11972688]\n",
      " [0.14205375]\n",
      " [0.15876871]\n",
      " [0.14217656]\n",
      " [0.1387872 ]\n",
      " [0.12983855]\n",
      " [0.17342969]\n",
      " [0.13294622]\n",
      " [0.34399084]\n",
      " [0.13154594]\n",
      " [0.11498487]\n",
      " [0.13664113]\n",
      " [0.1065985 ]\n",
      " [0.10913848]\n",
      " [0.20626357]\n",
      " [0.14515856]\n",
      " [0.16879937]\n",
      " [0.13702234]\n",
      " [0.22173289]\n",
      " [0.14601131]\n",
      " [0.33980111]\n",
      " [0.1894379 ]\n",
      " [0.16974472]\n",
      " [0.12714304]\n",
      " [0.10725385]\n",
      " [0.14425441]\n",
      " [0.16183579]\n",
      " [0.13911842]\n",
      " [0.20704462]\n",
      " [0.143149  ]\n",
      " [0.1826494 ]\n",
      " [0.12374273]\n",
      " [0.27620592]\n",
      " [0.13617468]\n",
      " [0.1427186 ]\n",
      " [0.11086915]\n",
      " [0.25583245]\n",
      " [0.0971596 ]\n",
      " [0.14750283]\n",
      " [0.17004217]\n",
      " [0.15843242]\n",
      " [0.12288901]\n",
      " [0.09925446]\n",
      " [0.16623264]\n",
      " [0.14175064]\n",
      " [0.15835499]\n",
      " [0.13203906]\n",
      " [0.18819959]\n",
      " [0.13654256]\n",
      " [0.18528988]\n",
      " [0.17650975]\n",
      " [0.17480588]\n",
      " [0.14972541]\n",
      " [0.23234765]\n",
      " [0.12118175]\n",
      " [0.13507636]\n",
      " [0.14895101]\n",
      " [0.12732893]\n",
      " [0.14553759]\n",
      " [0.1948828 ]\n",
      " [0.17129396]\n",
      " [0.13633285]\n",
      " [0.12832823]\n",
      " [0.11628825]\n",
      " [0.23216989]\n",
      " [0.1547608 ]\n",
      " [0.16291042]\n",
      " [0.12687481]\n",
      " [0.12648151]\n",
      " [0.16292576]\n",
      " [0.10770404]\n",
      " [0.18179463]\n",
      " [0.14171244]\n",
      " [0.10523594]\n",
      " [0.16371669]\n",
      " [0.17347508]\n",
      " [0.1599329 ]\n",
      " [0.14011859]\n",
      " [0.17585522]\n",
      " [0.11520521]\n",
      " [0.2488275 ]\n",
      " [0.11100595]\n",
      " [0.1813495 ]\n",
      " [0.15812234]\n",
      " [0.12901714]\n",
      " [0.15585362]\n",
      " [0.25674097]\n",
      " [0.11189161]\n",
      " [0.1430409 ]\n",
      " [0.09780995]\n",
      " [0.09922808]\n",
      " [0.10000919]\n",
      " [0.29686907]\n",
      " [0.12710909]\n",
      " [0.1220472 ]\n",
      " [0.28954987]\n",
      " [0.18625242]\n",
      " [0.14162045]\n",
      " [0.21600199]\n",
      " [0.12700613]\n",
      " [0.16562845]\n",
      " [0.14965112]\n",
      " [0.09261988]\n",
      " [0.12616687]\n",
      " [0.11942151]\n",
      " [0.12931138]\n",
      " [0.09504399]\n",
      " [0.10228823]\n",
      " [0.17796175]\n",
      " [0.18167237]\n",
      " [0.17095753]\n",
      " [0.14218591]\n",
      " [0.21464247]\n",
      " [0.26675415]\n",
      " [0.16783059]\n",
      " [0.27868633]\n",
      " [0.21496443]\n",
      " [0.17646463]\n",
      " [0.14279927]\n",
      " [0.13670893]\n",
      " [0.09174654]\n",
      " [0.14647835]\n",
      " [0.11401758]\n",
      " [0.14379977]\n",
      " [0.25384235]\n",
      " [0.18591123]\n",
      " [0.09372919]\n",
      " [0.12644282]\n",
      " [0.12713441]\n",
      " [0.12857116]\n",
      " [0.11398356]\n",
      " [0.13036483]\n",
      " [0.12866554]\n",
      " [0.13443231]\n",
      " [0.14912086]\n",
      " [0.181216  ]\n",
      " [0.14795494]\n",
      " [0.20147686]\n",
      " [0.11796418]\n",
      " [0.13777861]\n",
      " [0.13581758]\n",
      " [0.39902254]\n",
      " [0.34925735]\n",
      " [0.25123195]\n",
      " [0.11751086]\n",
      " [0.14492154]\n",
      " [0.14262742]\n",
      " [0.12658566]\n",
      " [0.14578115]\n",
      " [0.10462668]\n",
      " [0.14486902]\n",
      " [0.12959053]\n",
      " [0.15007877]\n",
      " [0.1301714 ]\n",
      " [0.13787347]\n",
      " [0.14671267]\n",
      " [0.12063524]\n",
      " [0.17850849]\n",
      " [0.12368053]\n",
      " [0.25957587]\n",
      " [0.12027748]\n",
      " [0.28067941]\n",
      " [0.15934562]\n",
      " [0.13717017]\n",
      " [0.125741  ]\n",
      " [0.13480938]\n",
      " [0.13292685]\n",
      " [0.11675896]\n",
      " [0.11799815]\n",
      " [0.11193044]\n",
      " [0.12465836]\n",
      " [0.11296754]\n",
      " [0.11096649]\n",
      " [0.1294579 ]\n",
      " [0.17345139]\n",
      " [0.14435202]\n",
      " [0.11376648]\n",
      " [0.34201851]\n",
      " [0.15975518]\n",
      " [0.13527144]\n",
      " [0.09492572]\n",
      " [0.17548262]\n",
      " [0.12001804]\n",
      " [0.11421317]\n",
      " [0.11472156]\n",
      " [0.14650588]\n",
      " [0.12965614]\n",
      " [0.13015611]\n",
      " [0.17182674]\n",
      " [0.12090028]\n",
      " [0.15297381]\n",
      " [0.26778201]\n",
      " [0.11140648]\n",
      " [0.15847925]\n",
      " [0.1278557 ]\n",
      " [0.1142071 ]\n",
      " [0.16063117]\n",
      " [0.16745972]\n",
      " [0.13979808]\n",
      " [0.2326038 ]\n",
      " [0.15153607]\n",
      " [0.14182597]\n",
      " [0.13320973]\n",
      " [0.18663169]\n",
      " [0.25256289]\n",
      " [0.17205472]\n",
      " [0.14190274]\n",
      " [0.1475391 ]\n",
      " [0.14026703]\n",
      " [0.12100215]\n",
      " [0.23482805]\n",
      " [0.21694523]\n",
      " [0.12403864]\n",
      " [0.1380652 ]\n",
      " [0.15926179]\n",
      " [0.21031461]\n",
      " [0.11072879]\n",
      " [0.1076924 ]\n",
      " [0.12591965]\n",
      " [0.12664435]\n",
      " [0.1981323 ]\n",
      " [0.24845892]\n",
      " [0.20515101]\n",
      " [0.09488849]\n",
      " [0.12239082]\n",
      " [0.26099679]\n",
      " [0.20907685]\n",
      " [0.16308296]\n",
      " [0.13724402]\n",
      " [0.12593786]\n",
      " [0.2301207 ]\n",
      " [0.13392859]\n",
      " [0.16576791]\n",
      " [0.16531647]\n",
      " [0.17037037]\n",
      " [0.14573972]\n",
      " [0.11556153]\n",
      " [0.23006495]\n",
      " [0.09192005]\n",
      " [0.10789088]\n",
      " [0.10650621]\n",
      " [0.1285111 ]\n",
      " [0.15832043]\n",
      " [0.11324048]\n",
      " [0.17273485]\n",
      " [0.21916638]\n",
      " [0.17122727]\n",
      " [0.13878114]\n",
      " [0.19547496]\n",
      " [0.25126217]\n",
      " [0.13435133]\n",
      " [0.15534327]\n",
      " [0.10210549]\n",
      " [0.17880258]\n",
      " [0.18030097]\n",
      " [0.19601995]\n",
      " [0.1208292 ]\n",
      " [0.12520293]\n",
      " [0.14273992]\n",
      " [0.10316795]\n",
      " [0.10899083]\n",
      " [0.13570123]\n",
      " [0.10189774]\n",
      " [0.14051388]\n",
      " [0.22986148]\n",
      " [0.21266873]\n",
      " [0.10657354]\n",
      " [0.09045853]\n",
      " [0.11395049]\n",
      " [0.1566095 ]\n",
      " [0.13188046]\n",
      " [0.12011296]\n",
      " [0.11752699]\n",
      " [0.16497791]\n",
      " [0.13304486]\n",
      " [0.10377341]\n",
      " [0.21482801]\n",
      " [0.12690645]\n",
      " [0.14476134]\n",
      " [0.22337094]\n",
      " [0.23747997]\n",
      " [0.10371406]\n",
      " [0.08585443]\n",
      " [0.11927636]\n",
      " [0.13109964]\n",
      " [0.14962389]\n",
      " [0.13334648]\n",
      " [0.11839537]\n",
      " [0.24950511]\n",
      " [0.12979727]\n",
      " [0.09641781]\n",
      " [0.15257182]\n",
      " [0.14393859]\n",
      " [0.10494579]\n",
      " [0.16582481]\n",
      " [0.1283801 ]\n",
      " [0.10056235]\n",
      " [0.09535924]\n",
      " [0.15884786]\n",
      " [0.30321687]\n",
      " [0.16725268]\n",
      " [0.13030443]\n",
      " [0.16162457]\n",
      " [0.1017121 ]\n",
      " [0.11313456]\n",
      " [0.20528712]\n",
      " [0.09653541]\n",
      " [0.09683022]\n",
      " [0.30192272]\n",
      " [0.10259088]\n",
      " [0.21811479]\n",
      " [0.14735999]\n",
      " [0.14684177]\n",
      " [0.21724328]\n",
      " [0.12366849]\n",
      " [0.1545729 ]\n",
      " [0.39133847]\n",
      " [0.18686992]\n",
      " [0.20036454]\n",
      " [0.11911766]\n",
      " [0.13454528]\n",
      " [0.16234889]\n",
      " [0.13589707]\n",
      " [0.1030693 ]\n",
      " [0.19921905]\n",
      " [0.12865495]\n",
      " [0.15004174]\n",
      " [0.11782577]\n",
      " [0.26372858]\n",
      " [0.11339889]\n",
      " [0.14740514]\n",
      " [0.20740829]\n",
      " [0.16602215]\n",
      " [0.09235195]\n",
      " [0.1730246 ]\n",
      " [0.20904975]\n",
      " [0.13790382]\n",
      " [0.12159148]\n",
      " [0.1058895 ]\n",
      " [0.17186049]\n",
      " [0.14108306]\n",
      " [0.11831809]\n",
      " [0.14174536]\n",
      " [0.12081483]\n",
      " [0.11377794]\n",
      " [0.21027681]\n",
      " [0.17949957]\n",
      " [0.35935984]\n",
      " [0.14231271]\n",
      " [0.14080287]\n",
      " [0.13010348]\n",
      " [0.11675523]\n",
      " [0.12624817]\n",
      " [0.09417143]\n",
      " [0.1631514 ]\n",
      " [0.14648202]\n",
      " [0.21125887]\n",
      " [0.13525139]\n",
      " [0.17888204]\n",
      " [0.15225676]\n",
      " [0.09658898]\n",
      " [0.15030841]\n",
      " [0.16390866]\n",
      " [0.10087961]\n",
      " [0.12865793]\n",
      " [0.12929569]\n",
      " [0.10101712]\n",
      " [0.22279169]\n",
      " [0.1635293 ]\n",
      " [0.17262349]\n",
      " [0.12778872]\n",
      " [0.11913156]\n",
      " [0.1083772 ]\n",
      " [0.13279989]\n",
      " [0.18232867]\n",
      " [0.12051232]\n",
      " [0.15887478]\n",
      " [0.14711825]\n",
      " [0.16570177]\n",
      " [0.1394026 ]\n",
      " [0.14466866]\n",
      " [0.129897  ]\n",
      " [0.12983779]\n",
      " [0.15610727]\n",
      " [0.13057912]\n",
      " [0.12624852]\n",
      " [0.09830361]\n",
      " [0.15093214]\n",
      " [0.13616923]\n",
      " [0.16097813]\n",
      " [0.11312237]\n",
      " [0.17140481]\n",
      " [0.14533427]\n",
      " [0.13023149]\n",
      " [0.11842532]\n",
      " [0.22505614]\n",
      " [0.14333472]\n",
      " [0.10465653]\n",
      " [0.11786744]\n",
      " [0.23305436]\n",
      " [0.14112524]\n",
      " [0.17637787]\n",
      " [0.12795732]\n",
      " [0.1147492 ]\n",
      " [0.14835552]\n",
      " [0.28465645]\n",
      " [0.14481236]\n",
      " [0.13971188]]\n",
      "\n",
      "Loss: \n",
      " 0.12136098950033136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset from CSV\n",
    "df = pd.read_csv(\"student_dataset.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = df.drop(columns=[\"Final_Score\"]).values  # All columns except target\n",
    "y = df[\"Final_Score\"].values.reshape(-1, 1)  # Target column\n",
    "\n",
    "# Normalize features\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100  # Normalize target\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        self.inputSize = X.shape[1]\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 8  # Increased hidden layer size for better learning\n",
    "        \n",
    "        # Weights\n",
    "        self.W1 = np.random.randn(self.inputSize, self.hiddenSize)  # Weight matrix from input to hidden layer\n",
    "        self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # Weight matrix from hidden to output layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = np.dot(X, self.W1)        # Dot product of input and first set of weights\n",
    "        self.z2 = self.sigmoid(self.z)     # Activation function\n",
    "        self.z3 = np.dot(self.z2, self.W2) # Dot product of hidden layer and second set of weights\n",
    "        o = self.sigmoid(self.z3)          # Final activation\n",
    "        return o\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)  # Derivative of sigmoid\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o  # Error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)  # Applying derivative of sigmoid to output error\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.W2.T)  # Error in hidden layer\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)  # Applying derivative of sigmoid\n",
    "        \n",
    "        self.W1 += X.T.dot(self.z2_delta)  # Adjust input -> hidden weights\n",
    "        self.W2 += self.z2.T.dot(self.o_delta)  # Adjust hidden -> output weights\n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "NN = Neural_Network()\n",
    "print(\"\\nInput: \\n\", X)\n",
    "print(\"\\nActual Output: \\n\", y)\n",
    "print(\"\\nPredicted Output: \\n\", NN.forward(X))\n",
    "print(\"\\nLoss: \\n\", np.mean(np.square(y - NN.forward(X))))  # Mean squared loss\n",
    "\n",
    "NN.train(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faea857e-38f3-4fdb-bb31-644bdccc984a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W1, W2, o\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m W1, W2, predictions \u001b[38;5;241m=\u001b[39m train(X, y)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, X)\n",
      "Cell \u001b[1;32mIn[2], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X, y, hidden_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     61\u001b[0m     z, z2, z3, o \u001b[38;5;241m=\u001b[39m forward_pass(X, W1, W2)\n\u001b[1;32m---> 62\u001b[0m     W1, W2 \u001b[38;5;241m=\u001b[39m backward_pass(X, y, W1, W2, z, z2, z3, o, learning_rate)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m W1, W2, o\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mbackward_pass\u001b[1;34m(X, y, W1, W2, z, z2, z3, o, learning_rate)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W1)):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W1[i])):\n\u001b[1;32m---> 47\u001b[0m         W1[i][j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(X[k][i] \u001b[38;5;241m*\u001b[39m z2_delta[k][j] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)))\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W2)):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W2[i])):\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W1)):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W1[i])):\n\u001b[1;32m---> 47\u001b[0m         W1[i][j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(X[k][i] \u001b[38;5;241m*\u001b[39m z2_delta[k][j] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)))\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W2)):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W2[i])):\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Load dataset manually\n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "    return [[float(value) for value in row] for row in data[1:]]  # Skip header\n",
    "\n",
    "data = load_csv(\"student_dataset.csv\")\n",
    "X = [row[:-1] for row in data]  # Features\n",
    "y = [[row[-1] / 100] for row in data]  # Normalize target\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    W1 = [[random.uniform(-1, 1) for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "    W2 = [[random.uniform(-1, 1) for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "    return W1, W2\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(X, W1, W2):\n",
    "    z = [[sum(i * w for i, w in zip(row, col)) for col in zip(*W1)] for row in X]\n",
    "    z2 = [[sigmoid(val) for val in row] for row in z]\n",
    "    z3 = [[sum(i * w for i, w in zip(row, col)) for col in zip(*W2)] for row in z2]\n",
    "    o = [[sigmoid(val) for val in row] for row in z3]\n",
    "    return z, z2, z3, o\n",
    "\n",
    "# Backpropagation\n",
    "def backward_pass(X, y, W1, W2, z, z2, z3, o, learning_rate=0.01):\n",
    "    o_error = [[yi - oi for yi, oi in zip(y_row, o_row)] for y_row, o_row in zip(y, o)]\n",
    "    o_delta = [[err * sigmoid_derivative(oi) for err, oi in zip(err_row, o_row)] for err_row, o_row in zip(o_error, o)]\n",
    "    \n",
    "    z2_error = [[sum(d * w for d, w in zip(delta_row, col)) for col in zip(*W2)] for delta_row in o_delta]\n",
    "    z2_delta = [[err * sigmoid_derivative(zi) for err, zi in zip(err_row, z2_row)] for err_row, z2_row in zip(z2_error, z2)]\n",
    "    \n",
    "    for i in range(len(W1)):\n",
    "        for j in range(len(W1[i])):\n",
    "            W1[i][j] += learning_rate * sum(X[k][i] * z2_delta[k][j] for k in range(len(X)))\n",
    "    \n",
    "    for i in range(len(W2)):\n",
    "        for j in range(len(W2[i])):\n",
    "            W2[i][j] += learning_rate * sum(z2[k][i] * o_delta[k][j] for k in range(len(z2)))\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# Training function\n",
    "def train(X, y, hidden_size=8, epochs=100, learning_rate=0.01):\n",
    "    input_size, output_size = len(X[0]), 1\n",
    "    W1, W2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        z, z2, z3, o = forward_pass(X, W1, W2)\n",
    "        W1, W2 = backward_pass(X, y, W1, W2, z, z2, z3, o, learning_rate)\n",
    "    \n",
    "    return W1, W2, o\n",
    "\n",
    "# Train the model\n",
    "W1, W2, predictions = train(X, y)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nInput: \\n\", X)\n",
    "print(\"\\nActual Output: \\n\", y)\n",
    "print(\"\\nPredicted Output: \\n\", predictions)\n",
    "print(\"\\nLoss: \\n\", sum(sum((yi - oi) ** 2 for yi, oi in zip(y_row, o_row)) for y_row, o_row in zip(y, predictions)) / len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2187d41b-b724-4b4f-9b46-2ee70012f745",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W1, W2, o\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m W1, W2, predictions \u001b[38;5;241m=\u001b[39m train(X, y)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, X)\n",
      "Cell \u001b[1;32mIn[3], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X, y, hidden_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     61\u001b[0m     z, z2, z3, o \u001b[38;5;241m=\u001b[39m forward_pass(X, W1, W2)\n\u001b[1;32m---> 62\u001b[0m     W1, W2 \u001b[38;5;241m=\u001b[39m backward_pass(X, y, W1, W2, z, z2, z3, o, learning_rate)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m W1, W2, o\n",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m, in \u001b[0;36mbackward_pass\u001b[1;34m(X, y, W1, W2, z, z2, z3, o, learning_rate)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W1)):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W1[i])):\n\u001b[1;32m---> 47\u001b[0m         W1[i][j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(X[k][i] \u001b[38;5;241m*\u001b[39m z2_delta[k][j] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)))\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W2)):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W2[i])):\n",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W1)):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W1[i])):\n\u001b[1;32m---> 47\u001b[0m         W1[i][j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(X[k][i] \u001b[38;5;241m*\u001b[39m z2_delta[k][j] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)))\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W2)):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(W2[i])):\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Load dataset manually\n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "    return [[float(value) for value in row] for row in data[1:]]  # Skip header\n",
    "\n",
    "data = load_csv(\"student_dataset.csv\")\n",
    "X = [row[:-1] for row in data]  # Features\n",
    "y = [[row[-1] / 100] for row in data]  # Normalize target\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    W1 = [[random.uniform(-1, 1) for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "    W2 = [[random.uniform(-1, 1) for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "    return W1, W2\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(X, W1, W2):\n",
    "    z = [[sum(i * w for i, w in zip(row, col)) for col in zip(*W1)] for row in X]\n",
    "    z2 = [[sigmoid(val) for val in row] for row in z]\n",
    "    z3 = [[sum(i * w for i, w in zip(row, col)) for col in zip(*W2)] for row in z2]\n",
    "    o = [[sigmoid(val) for val in row] for row in z3]\n",
    "    return z, z2, z3, o\n",
    "\n",
    "# Backpropagation\n",
    "def backward_pass(X, y, W1, W2, z, z2, z3, o, learning_rate=0.01):\n",
    "    o_error = [[yi - oi for yi, oi in zip(y_row, o_row)] for y_row, o_row in zip(y, o)]\n",
    "    o_delta = [[err * sigmoid_derivative(oi) for err, oi in zip(err_row, o_row)] for err_row, o_row in zip(o_error, o)]\n",
    "    \n",
    "    z2_error = [[sum(d * w for d, w in zip(delta_row, col)) for col in zip(*W2)] for delta_row in o_delta]\n",
    "    z2_delta = [[err * sigmoid_derivative(zi) for err, zi in zip(err_row, z2_row)] for err_row, z2_row in zip(z2_error, z2)]\n",
    "    \n",
    "    for i in range(len(W1)):\n",
    "        for j in range(len(W1[i])):\n",
    "            W1[i][j] += learning_rate * sum(X[k][i] * z2_delta[k][j] for k in range(len(X)))\n",
    "    \n",
    "    for i in range(len(W2)):\n",
    "        for j in range(len(W2[i])):\n",
    "            W2[i][j] += learning_rate * sum(z2[k][i] * o_delta[k][j] for k in range(len(z2)))\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# Training function\n",
    "def train(X, y, hidden_size=8, epochs=100, learning_rate=0.01):\n",
    "    input_size, output_size = len(X[0]), 1\n",
    "    W1, W2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        z, z2, z3, o = forward_pass(X, W1, W2)\n",
    "        W1, W2 = backward_pass(X, y, W1, W2, z, z2, z3, o, learning_rate)\n",
    "    \n",
    "    return W1, W2, o\n",
    "\n",
    "# Train the model\n",
    "W1, W2, predictions = train(X, y)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nInput: \\n\", X)\n",
    "print(\"\\nActual Output: \\n\", y)\n",
    "print(\"\\nPredicted Output: \\n\", predictions)\n",
    "print(\"\\nLoss: \\n\", sum(sum((yi - oi) ** 2 for yi, oi in zip(y_row, o_row)) for y_row, o_row in zip(y, predictions)) / len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73498831-e3dd-4681-805f-04fccf418f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 1.2123073849663283e-05\n",
      "Epoch 0, Loss: 1.2123073849663283e-05\n",
      "Epoch 1000, Loss: 1.795663004120935e-11\n",
      "Epoch 2000, Loss: 1.785839046423548e-11\n",
      "Epoch 3000, Loss: 1.7760957453540117e-11\n",
      "Epoch 4000, Loss: 1.766432152361292e-11\n",
      "Epoch 5000, Loss: 1.756847333639053e-11\n",
      "Epoch 6000, Loss: 1.7473403698292088e-11\n",
      "Epoch 7000, Loss: 1.7379103557859234e-11\n",
      "Epoch 8000, Loss: 1.728556400255743e-11\n",
      "Epoch 9000, Loss: 1.7192776256603524e-11\n",
      "\n",
      "Final Predicted Output:\n",
      " [[0.49999596]\n",
      " [0.50000436]\n",
      " [0.49999749]\n",
      " [0.50000244]\n",
      " [0.50000372]\n",
      " [0.49999861]\n",
      " [0.50000171]\n",
      " [0.50000457]\n",
      " [0.50000494]\n",
      " [0.49999782]\n",
      " [0.49999807]\n",
      " [0.50000092]\n",
      " [0.50000344]\n",
      " [0.50000042]\n",
      " [0.49999998]\n",
      " [0.50000676]\n",
      " [0.50000485]\n",
      " [0.49999424]\n",
      " [0.49999935]\n",
      " [0.5000003 ]\n",
      " [0.49999132]\n",
      " [0.50000261]\n",
      " [0.5000027 ]\n",
      " [0.50000182]\n",
      " [0.50000221]\n",
      " [0.50000433]\n",
      " [0.50000006]\n",
      " [0.49999585]\n",
      " [0.50000266]\n",
      " [0.49999168]\n",
      " [0.49999351]\n",
      " [0.50000053]\n",
      " [0.50000789]\n",
      " [0.49999923]\n",
      " [0.50000098]\n",
      " [0.50000132]\n",
      " [0.50000413]\n",
      " [0.50000273]\n",
      " [0.50000867]\n",
      " [0.49999807]\n",
      " [0.50000084]\n",
      " [0.50000654]\n",
      " [0.50000188]\n",
      " [0.50000149]\n",
      " [0.49999739]\n",
      " [0.50000321]\n",
      " [0.49999357]\n",
      " [0.49999861]\n",
      " [0.50000193]\n",
      " [0.49999807]\n",
      " [0.50000197]\n",
      " [0.50000184]\n",
      " [0.50000268]\n",
      " [0.50000277]\n",
      " [0.50000313]\n",
      " [0.50000644]\n",
      " [0.4999967 ]\n",
      " [0.49999991]\n",
      " [0.49999215]\n",
      " [0.50000002]\n",
      " [0.5000001 ]\n",
      " [0.50000267]\n",
      " [0.49999884]\n",
      " [0.49999327]\n",
      " [0.49999971]\n",
      " [0.50000353]\n",
      " [0.49999813]\n",
      " [0.50000096]\n",
      " [0.50000404]\n",
      " [0.49999774]\n",
      " [0.50000728]\n",
      " [0.49999779]\n",
      " [0.50000387]\n",
      " [0.49999834]\n",
      " [0.50000045]\n",
      " [0.50000329]\n",
      " [0.50000882]\n",
      " [0.49998663]\n",
      " [0.50000178]\n",
      " [0.49998706]\n",
      " [0.50000594]\n",
      " [0.50000283]\n",
      " [0.50000362]\n",
      " [0.50000034]\n",
      " [0.49999934]\n",
      " [0.49999917]\n",
      " [0.50000385]\n",
      " [0.49999065]\n",
      " [0.49999756]\n",
      " [0.50000322]\n",
      " [0.50000145]\n",
      " [0.49999948]\n",
      " [0.49999927]\n",
      " [0.49999852]\n",
      " [0.50000233]\n",
      " [0.49999487]\n",
      " [0.50000604]\n",
      " [0.49999946]\n",
      " [0.50000137]\n",
      " [0.49998653]\n",
      " [0.49999433]\n",
      " [0.5000015 ]\n",
      " [0.50000352]\n",
      " [0.5000041 ]\n",
      " [0.50000005]\n",
      " [0.49999704]\n",
      " [0.50000091]\n",
      " [0.49999302]\n",
      " [0.50000007]\n",
      " [0.500001  ]\n",
      " [0.49999713]\n",
      " [0.50000261]\n",
      " [0.50000031]\n",
      " [0.49999628]\n",
      " [0.50000236]\n",
      " [0.49999828]\n",
      " [0.49999676]\n",
      " [0.49999633]\n",
      " [0.50000143]\n",
      " [0.50000687]\n",
      " [0.50000493]\n",
      " [0.50000288]\n",
      " [0.50000635]\n",
      " [0.50000015]\n",
      " [0.50000239]\n",
      " [0.4999944 ]\n",
      " [0.50000118]\n",
      " [0.50000327]\n",
      " [0.50000101]\n",
      " [0.50000326]\n",
      " [0.50000495]\n",
      " [0.50000652]\n",
      " [0.49999877]\n",
      " [0.49999731]\n",
      " [0.49999634]\n",
      " [0.50000689]\n",
      " [0.49999903]\n",
      " [0.50000238]\n",
      " [0.50000029]\n",
      " [0.50000373]\n",
      " [0.50000064]\n",
      " [0.49999931]\n",
      " [0.50000241]\n",
      " [0.49999679]\n",
      " [0.49999943]\n",
      " [0.50000115]\n",
      " [0.50000504]\n",
      " [0.4999932 ]\n",
      " [0.50000135]\n",
      " [0.50000115]\n",
      " [0.49999677]\n",
      " [0.49999811]\n",
      " [0.49998516]\n",
      " [0.50000308]\n",
      " [0.50000081]\n",
      " [0.49999478]\n",
      " [0.49999959]\n",
      " [0.5000004 ]\n",
      " [0.50000469]\n",
      " [0.49999951]\n",
      " [0.49999784]\n",
      " [0.50000227]\n",
      " [0.50000122]\n",
      " [0.50000032]\n",
      " [0.49999549]\n",
      " [0.50000388]\n",
      " [0.50000255]\n",
      " [0.50000492]\n",
      " [0.49999682]\n",
      " [0.49999953]\n",
      " [0.5000053 ]\n",
      " [0.50000016]\n",
      " [0.49999812]\n",
      " [0.49999783]\n",
      " [0.49999868]\n",
      " [0.50000188]\n",
      " [0.50000215]\n",
      " [0.50001053]\n",
      " [0.50000396]\n",
      " [0.5000034 ]\n",
      " [0.50000113]\n",
      " [0.50000312]\n",
      " [0.50000293]\n",
      " [0.50000231]\n",
      " [0.50000669]\n",
      " [0.4999993 ]\n",
      " [0.50000513]\n",
      " [0.49999585]\n",
      " [0.5000031 ]\n",
      " [0.49999399]\n",
      " [0.49999645]\n",
      " [0.50000247]\n",
      " [0.50000543]\n",
      " [0.4999992 ]\n",
      " [0.50000753]\n",
      " [0.50000012]\n",
      " [0.50000348]\n",
      " [0.49999807]\n",
      " [0.50000003]\n",
      " [0.50000495]\n",
      " [0.50000354]\n",
      " [0.50000298]\n",
      " [0.5000022 ]\n",
      " [0.49999659]\n",
      " [0.50000209]\n",
      " [0.50000329]\n",
      " [0.50000112]\n",
      " [0.500006  ]\n",
      " [0.49999268]\n",
      " [0.50000219]\n",
      " [0.50000021]\n",
      " [0.50000415]\n",
      " [0.50000375]\n",
      " [0.50000814]\n",
      " [0.5000054 ]\n",
      " [0.49999768]\n",
      " [0.50000516]\n",
      " [0.49999557]\n",
      " [0.50000303]\n",
      " [0.50000288]\n",
      " [0.49999947]\n",
      " [0.50000231]\n",
      " [0.50000758]\n",
      " [0.50000629]\n",
      " [0.50000114]\n",
      " [0.50000339]\n",
      " [0.500001  ]\n",
      " [0.4999965 ]\n",
      " [0.49999878]\n",
      " [0.50000012]\n",
      " [0.50000125]\n",
      " [0.50000068]\n",
      " [0.50000633]\n",
      " [0.49999859]\n",
      " [0.49999996]\n",
      " [0.50000078]\n",
      " [0.50000279]\n",
      " [0.50000367]\n",
      " [0.50000124]\n",
      " [0.50000073]\n",
      " [0.50000336]\n",
      " [0.49999875]\n",
      " [0.50000153]\n",
      " [0.49999628]\n",
      " [0.4999977 ]\n",
      " [0.50000608]\n",
      " [0.49999547]\n",
      " [0.50000212]\n",
      " [0.5000039 ]\n",
      " [0.50000453]\n",
      " [0.49999887]\n",
      " [0.50000315]\n",
      " [0.50000721]\n",
      " [0.50000304]\n",
      " [0.49999653]\n",
      " [0.50000389]\n",
      " [0.50000435]\n",
      " [0.49999926]\n",
      " [0.50000565]\n",
      " [0.49999498]\n",
      " [0.4999926 ]\n",
      " [0.50000304]\n",
      " [0.50000046]\n",
      " [0.49999968]\n",
      " [0.50000354]\n",
      " [0.50000191]\n",
      " [0.49999608]\n",
      " [0.49999864]\n",
      " [0.50000363]\n",
      " [0.50000381]\n",
      " [0.49999775]\n",
      " [0.50000066]\n",
      " [0.50000342]\n",
      " [0.49999896]\n",
      " [0.49999829]\n",
      " [0.50000256]\n",
      " [0.49999687]\n",
      " [0.5000049 ]\n",
      " [0.50000167]\n",
      " [0.50000469]\n",
      " [0.50000256]\n",
      " [0.50000444]\n",
      " [0.50000316]\n",
      " [0.49998807]\n",
      " [0.49999636]\n",
      " [0.50000214]\n",
      " [0.50000434]\n",
      " [0.5000039 ]\n",
      " [0.49999907]\n",
      " [0.50000676]\n",
      " [0.5000034 ]\n",
      " [0.50000256]\n",
      " [0.50000108]\n",
      " [0.49999632]\n",
      " [0.50000239]\n",
      " [0.50000275]\n",
      " [0.50000228]\n",
      " [0.49999759]\n",
      " [0.49999922]\n",
      " [0.49999933]\n",
      " [0.49999803]\n",
      " [0.50000349]\n",
      " [0.50000135]\n",
      " [0.4999971 ]\n",
      " [0.50000135]\n",
      " [0.50000379]\n",
      " [0.50000411]\n",
      " [0.49999022]\n",
      " [0.50000114]\n",
      " [0.50000495]\n",
      " [0.50000394]\n",
      " [0.49999839]\n",
      " [0.4999962 ]\n",
      " [0.50000501]\n",
      " [0.499999  ]\n",
      " [0.49999876]\n",
      " [0.49999121]\n",
      " [0.50000133]\n",
      " [0.49999632]\n",
      " [0.49999533]\n",
      " [0.50000133]\n",
      " [0.50000358]\n",
      " [0.49999661]\n",
      " [0.49999558]\n",
      " [0.50000034]\n",
      " [0.50000646]\n",
      " [0.50000767]\n",
      " [0.49999648]\n",
      " [0.49999643]\n",
      " [0.50000371]\n",
      " [0.49999983]\n",
      " [0.50000849]\n",
      " [0.49999566]\n",
      " [0.49999491]\n",
      " [0.50000066]\n",
      " [0.49999849]\n",
      " [0.50000225]\n",
      " [0.49999578]\n",
      " [0.4999967 ]\n",
      " [0.49998801]\n",
      " [0.49999783]\n",
      " [0.50000568]\n",
      " [0.500003  ]\n",
      " [0.50000167]\n",
      " [0.49999122]\n",
      " [0.50000291]\n",
      " [0.50000133]\n",
      " [0.49999308]\n",
      " [0.49999505]\n",
      " [0.49999928]\n",
      " [0.49999644]\n",
      " [0.5000019 ]\n",
      " [0.50000632]\n",
      " [0.50000131]\n",
      " [0.49998634]\n",
      " [0.49998918]\n",
      " [0.4999925 ]\n",
      " [0.49998637]\n",
      " [0.50000458]\n",
      " [0.5000022 ]\n",
      " [0.50000753]\n",
      " [0.5000012 ]\n",
      " [0.49999941]\n",
      " [0.49999873]\n",
      " [0.49999583]\n",
      " [0.50000338]\n",
      " [0.50000114]\n",
      " [0.49999641]\n",
      " [0.50000209]\n",
      " [0.49999944]\n",
      " [0.49999907]\n",
      " [0.49999416]\n",
      " [0.49999196]\n",
      " [0.5000039 ]\n",
      " [0.5000049 ]\n",
      " [0.50000147]\n",
      " [0.50000165]\n",
      " [0.49999829]\n",
      " [0.50000043]\n",
      " [0.50000747]\n",
      " [0.49999976]\n",
      " [0.50000386]\n",
      " [0.50000224]\n",
      " [0.49999345]\n",
      " [0.50000604]\n",
      " [0.5000015 ]\n",
      " [0.50000768]\n",
      " [0.50000054]\n",
      " [0.50000591]\n",
      " [0.50000129]\n",
      " [0.50000162]\n",
      " [0.49999797]\n",
      " [0.50000213]\n",
      " [0.50000627]\n",
      " [0.49999875]\n",
      " [0.50000183]\n",
      " [0.50000783]\n",
      " [0.50000332]\n",
      " [0.50000244]\n",
      " [0.50000373]\n",
      " [0.49999617]\n",
      " [0.49999191]\n",
      " [0.5000035 ]\n",
      " [0.49999846]\n",
      " [0.50000039]\n",
      " [0.50000529]\n",
      " [0.5000026 ]\n",
      " [0.49999884]\n",
      " [0.49999776]\n",
      " [0.49999882]\n",
      " [0.50000224]\n",
      " [0.4999972 ]\n",
      " [0.49999514]\n",
      " [0.50000107]\n",
      " [0.4999987 ]\n",
      " [0.49999913]\n",
      " [0.49999963]\n",
      " [0.49999753]\n",
      " [0.50000048]\n",
      " [0.49999275]\n",
      " [0.49999948]\n",
      " [0.50000418]\n",
      " [0.49998975]\n",
      " [0.49999672]\n",
      " [0.49999517]\n",
      " [0.50000263]\n",
      " [0.50000656]\n",
      " [0.50000068]\n",
      " [0.49999735]\n",
      " [0.50000435]\n",
      " [0.49999464]\n",
      " [0.50000208]\n",
      " [0.49999648]\n",
      " [0.5000043 ]\n",
      " [0.49999682]\n",
      " [0.50000035]\n",
      " [0.49999719]\n",
      " [0.49999859]\n",
      " [0.50000471]\n",
      " [0.49999579]\n",
      " [0.50000287]\n",
      " [0.50000803]\n",
      " [0.50000371]\n",
      " [0.50000209]\n",
      " [0.50000237]\n",
      " [0.5000014 ]\n",
      " [0.50000351]\n",
      " [0.50000476]\n",
      " [0.50000138]\n",
      " [0.49999974]\n",
      " [0.50000398]\n",
      " [0.50000602]\n",
      " [0.49999618]\n",
      " [0.50000232]\n",
      " [0.50000222]\n",
      " [0.50000637]\n",
      " [0.49999932]\n",
      " [0.50000286]\n",
      " [0.49999268]\n",
      " [0.49999849]\n",
      " [0.49999841]\n",
      " [0.49999541]\n",
      " [0.49999724]\n",
      " [0.50000074]\n",
      " [0.49999854]\n",
      " [0.49999974]\n",
      " [0.49999712]\n",
      " [0.50000117]\n",
      " [0.50000692]\n",
      " [0.49999928]\n",
      " [0.50000777]\n",
      " [0.5000018 ]\n",
      " [0.4999969 ]\n",
      " [0.49999704]\n",
      " [0.50000001]\n",
      " [0.50000372]\n",
      " [0.49999448]\n",
      " [0.49999856]\n",
      " [0.50000302]\n",
      " [0.49999899]\n",
      " [0.50000107]\n",
      " [0.50000348]\n",
      " [0.49999514]\n",
      " [0.49999671]\n",
      " [0.49999864]\n",
      " [0.50000126]\n",
      " [0.50000277]\n",
      " [0.50000538]\n",
      " [0.50000589]\n",
      " [0.50000272]\n",
      " [0.50000133]\n",
      " [0.4999986 ]\n",
      " [0.49999628]\n",
      " [0.49999972]\n",
      " [0.50000527]\n",
      " [0.50000103]\n",
      " [0.50000372]\n",
      " [0.4999971 ]\n",
      " [0.50000506]\n",
      " [0.50000239]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset from CSV\n",
    "df = pd.read_csv(\"student_dataset.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = df.drop(columns=[\"Final_Score\"]).values  # All columns except target\n",
    "y = df[\"Final_Score\"].values.reshape(-1, 1)  # Target column\n",
    "\n",
    "# Normalize features\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100  # Normalize target\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Weights initialization\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_derivative(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1)\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2)\n",
    "        self.output = self.sigmoid(self.z2)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        output_error = y - self.output  # Error in output\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output)  # Applying derivative of sigmoid\n",
    "        \n",
    "        hidden_error = output_delta.dot(self.W2.T)  # Error in hidden layer\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.a1)  # Applying derivative of sigmoid\n",
    "        \n",
    "        # Weight updates\n",
    "        self.W2 += self.a1.T.dot(output_delta) * self.learning_rate\n",
    "        self.W1 += X.T.dot(hidden_delta) * self.learning_rate\n",
    "    \n",
    "    def train(self, X, y, epochs=10000):\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean(np.square(y - self.output))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Initialize and train the neural network\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 8  # You can adjust this\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "NN = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "print(\"Initial Loss:\", np.mean(np.square(y - NN.forward(X))))\n",
    "\n",
    "NN.train(X, y)\n",
    "\n",
    "print(\"\\nFinal Predicted Output:\\n\", NN.forward(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ded8a4-7c37-4e35-9c83-af922d0d8b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 2.3663190955024295e-05\n",
      "Epoch 0, Loss: 2.3663190955024295e-05\n",
      "Epoch 1000, Loss: 7.201122732359262e-13\n",
      "Epoch 2000, Loss: 7.144406016030393e-13\n",
      "Epoch 3000, Loss: 7.088284571559326e-13\n",
      "Epoch 4000, Loss: 7.032751199163663e-13\n",
      "Epoch 5000, Loss: 6.977798794151509e-13\n",
      "Epoch 6000, Loss: 6.923420345160721e-13\n",
      "Epoch 7000, Loss: 6.869608933166512e-13\n",
      "Epoch 8000, Loss: 6.816357729883843e-13\n",
      "Epoch 9000, Loss: 6.763659996801884e-13\n",
      "\n",
      "Final Predicted Output:\n",
      " [[0.50000015]\n",
      " [0.49999969]\n",
      " [0.50000183]\n",
      " [0.50000116]\n",
      " [0.5       ]\n",
      " [0.5000001 ]\n",
      " [0.4999997 ]\n",
      " [0.50000041]\n",
      " [0.49999957]\n",
      " [0.50000091]\n",
      " [0.49999902]\n",
      " [0.50000014]\n",
      " [0.50000017]\n",
      " [0.50000041]\n",
      " [0.49999921]\n",
      " [0.49999981]\n",
      " [0.49999947]\n",
      " [0.50000061]\n",
      " [0.50000183]\n",
      " [0.49999838]\n",
      " [0.50000051]\n",
      " [0.4999984 ]\n",
      " [0.50000097]\n",
      " [0.50000078]\n",
      " [0.50000076]\n",
      " [0.50000098]\n",
      " [0.49999989]\n",
      " [0.50000082]\n",
      " [0.49999775]\n",
      " [0.50000107]\n",
      " [0.49999922]\n",
      " [0.50000026]\n",
      " [0.49999968]\n",
      " [0.50000007]\n",
      " [0.50000127]\n",
      " [0.50000035]\n",
      " [0.50000036]\n",
      " [0.49999961]\n",
      " [0.49999892]\n",
      " [0.49999978]\n",
      " [0.50000002]\n",
      " [0.50000086]\n",
      " [0.49999768]\n",
      " [0.50000036]\n",
      " [0.49999943]\n",
      " [0.4999989 ]\n",
      " [0.4999987 ]\n",
      " [0.49999962]\n",
      " [0.49999951]\n",
      " [0.50000071]\n",
      " [0.49999927]\n",
      " [0.5000008 ]\n",
      " [0.5000009 ]\n",
      " [0.49999916]\n",
      " [0.50000132]\n",
      " [0.50000053]\n",
      " [0.49999904]\n",
      " [0.49999946]\n",
      " [0.49999963]\n",
      " [0.50000017]\n",
      " [0.5000005 ]\n",
      " [0.50000037]\n",
      " [0.49999845]\n",
      " [0.5000002 ]\n",
      " [0.50000071]\n",
      " [0.50000109]\n",
      " [0.50000062]\n",
      " [0.4999999 ]\n",
      " [0.49999967]\n",
      " [0.50000001]\n",
      " [0.49999925]\n",
      " [0.50000064]\n",
      " [0.49999881]\n",
      " [0.49999928]\n",
      " [0.50000217]\n",
      " [0.50000124]\n",
      " [0.4999995 ]\n",
      " [0.50000165]\n",
      " [0.49999902]\n",
      " [0.50000097]\n",
      " [0.5000006 ]\n",
      " [0.50000043]\n",
      " [0.49999945]\n",
      " [0.49999947]\n",
      " [0.49999976]\n",
      " [0.49999894]\n",
      " [0.4999995 ]\n",
      " [0.50000047]\n",
      " [0.49999949]\n",
      " [0.49999857]\n",
      " [0.50000004]\n",
      " [0.50000057]\n",
      " [0.5000012 ]\n",
      " [0.50000064]\n",
      " [0.49999928]\n",
      " [0.50000069]\n",
      " [0.50000165]\n",
      " [0.49999846]\n",
      " [0.50000043]\n",
      " [0.49999803]\n",
      " [0.50000114]\n",
      " [0.49999935]\n",
      " [0.50000047]\n",
      " [0.49999938]\n",
      " [0.5000006 ]\n",
      " [0.49999842]\n",
      " [0.49999979]\n",
      " [0.50000092]\n",
      " [0.50000063]\n",
      " [0.4999999 ]\n",
      " [0.49999969]\n",
      " [0.49999996]\n",
      " [0.50000037]\n",
      " [0.49999979]\n",
      " [0.50000008]\n",
      " [0.50000085]\n",
      " [0.49999811]\n",
      " [0.50000014]\n",
      " [0.49999956]\n",
      " [0.49999936]\n",
      " [0.49999964]\n",
      " [0.50000099]\n",
      " [0.4999985 ]\n",
      " [0.49999859]\n",
      " [0.50000043]\n",
      " [0.50000091]\n",
      " [0.49999961]\n",
      " [0.50000027]\n",
      " [0.49999835]\n",
      " [0.49999989]\n",
      " [0.49999966]\n",
      " [0.5000001 ]\n",
      " [0.50000028]\n",
      " [0.49999861]\n",
      " [0.49999905]\n",
      " [0.50000015]\n",
      " [0.50000023]\n",
      " [0.49999927]\n",
      " [0.49999796]\n",
      " [0.49999948]\n",
      " [0.4999994 ]\n",
      " [0.49999873]\n",
      " [0.49999959]\n",
      " [0.49999927]\n",
      " [0.49999951]\n",
      " [0.50000028]\n",
      " [0.49999987]\n",
      " [0.50000045]\n",
      " [0.49999878]\n",
      " [0.49999965]\n",
      " [0.49999898]\n",
      " [0.49999977]\n",
      " [0.50000131]\n",
      " [0.50000009]\n",
      " [0.50000257]\n",
      " [0.49999864]\n",
      " [0.50000045]\n",
      " [0.49999967]\n",
      " [0.50000046]\n",
      " [0.50000002]\n",
      " [0.50000086]\n",
      " [0.49999938]\n",
      " [0.49999917]\n",
      " [0.49999984]\n",
      " [0.4999985 ]\n",
      " [0.50000075]\n",
      " [0.49999929]\n",
      " [0.49999786]\n",
      " [0.49999982]\n",
      " [0.49999852]\n",
      " [0.49999995]\n",
      " [0.50000047]\n",
      " [0.50000015]\n",
      " [0.50000007]\n",
      " [0.49999949]\n",
      " [0.50000071]\n",
      " [0.49999937]\n",
      " [0.49999861]\n",
      " [0.50000088]\n",
      " [0.50000009]\n",
      " [0.50000115]\n",
      " [0.49999894]\n",
      " [0.50000023]\n",
      " [0.50000011]\n",
      " [0.49999979]\n",
      " [0.49999928]\n",
      " [0.49999979]\n",
      " [0.50000034]\n",
      " [0.50000119]\n",
      " [0.49999996]\n",
      " [0.49999988]\n",
      " [0.50000103]\n",
      " [0.5000005 ]\n",
      " [0.50000035]\n",
      " [0.49999915]\n",
      " [0.50000153]\n",
      " [0.50000075]\n",
      " [0.50000232]\n",
      " [0.4999999 ]\n",
      " [0.50000005]\n",
      " [0.49999949]\n",
      " [0.49999848]\n",
      " [0.49999939]\n",
      " [0.50000015]\n",
      " [0.49999979]\n",
      " [0.50000103]\n",
      " [0.49999956]\n",
      " [0.50000057]\n",
      " [0.49999881]\n",
      " [0.49999993]\n",
      " [0.50000102]\n",
      " [0.49999937]\n",
      " [0.49999928]\n",
      " [0.49999995]\n",
      " [0.50000041]\n",
      " [0.49999958]\n",
      " [0.49999991]\n",
      " [0.50000193]\n",
      " [0.49999926]\n",
      " [0.49999981]\n",
      " [0.50000093]\n",
      " [0.50000048]\n",
      " [0.49999897]\n",
      " [0.50000015]\n",
      " [0.49999966]\n",
      " [0.49999977]\n",
      " [0.5       ]\n",
      " [0.4999998 ]\n",
      " [0.49999909]\n",
      " [0.49999929]\n",
      " [0.50000031]\n",
      " [0.50000056]\n",
      " [0.49999924]\n",
      " [0.50000113]\n",
      " [0.50000058]\n",
      " [0.50000013]\n",
      " [0.49999938]\n",
      " [0.49999998]\n",
      " [0.50000044]\n",
      " [0.49999954]\n",
      " [0.5000005 ]\n",
      " [0.50000044]\n",
      " [0.50000024]\n",
      " [0.50000214]\n",
      " [0.49999931]\n",
      " [0.50000115]\n",
      " [0.50000009]\n",
      " [0.49999994]\n",
      " [0.50000099]\n",
      " [0.500001  ]\n",
      " [0.49999985]\n",
      " [0.50000158]\n",
      " [0.50000049]\n",
      " [0.49999912]\n",
      " [0.50000048]\n",
      " [0.49999864]\n",
      " [0.49999976]\n",
      " [0.50000051]\n",
      " [0.50000008]\n",
      " [0.50000073]\n",
      " [0.49999916]\n",
      " [0.49999912]\n",
      " [0.49999899]\n",
      " [0.5000005 ]\n",
      " [0.49999966]\n",
      " [0.49999973]\n",
      " [0.49999975]\n",
      " [0.5000002 ]\n",
      " [0.49999926]\n",
      " [0.50000082]\n",
      " [0.50000032]\n",
      " [0.5       ]\n",
      " [0.49999931]\n",
      " [0.50000004]\n",
      " [0.50000163]\n",
      " [0.49999932]\n",
      " [0.4999998 ]\n",
      " [0.50000021]\n",
      " [0.50000086]\n",
      " [0.49999959]\n",
      " [0.50000007]\n",
      " [0.49999939]\n",
      " [0.49999872]\n",
      " [0.49999992]\n",
      " [0.4999993 ]\n",
      " [0.49999998]\n",
      " [0.49999925]\n",
      " [0.49999948]\n",
      " [0.50000093]\n",
      " [0.49999954]\n",
      " [0.49999999]\n",
      " [0.49999967]\n",
      " [0.499999  ]\n",
      " [0.50000096]\n",
      " [0.49999987]\n",
      " [0.50000095]\n",
      " [0.50000004]\n",
      " [0.50000057]\n",
      " [0.50000105]\n",
      " [0.50000074]\n",
      " [0.49999932]\n",
      " [0.49999889]\n",
      " [0.49999905]\n",
      " [0.50000065]\n",
      " [0.49999947]\n",
      " [0.50000018]\n",
      " [0.50000026]\n",
      " [0.50000147]\n",
      " [0.49999965]\n",
      " [0.49999929]\n",
      " [0.50000093]\n",
      " [0.50000101]\n",
      " [0.50000001]\n",
      " [0.50000049]\n",
      " [0.50000202]\n",
      " [0.49999943]\n",
      " [0.50000049]\n",
      " [0.49999934]\n",
      " [0.49999988]\n",
      " [0.49999993]\n",
      " [0.49999999]\n",
      " [0.50000117]\n",
      " [0.50000055]\n",
      " [0.499999  ]\n",
      " [0.50000124]\n",
      " [0.50000094]\n",
      " [0.49999985]\n",
      " [0.4999991 ]\n",
      " [0.50000045]\n",
      " [0.49999991]\n",
      " [0.49999879]\n",
      " [0.4999987 ]\n",
      " [0.49999968]\n",
      " [0.50000036]\n",
      " [0.49999951]\n",
      " [0.50000048]\n",
      " [0.50000059]\n",
      " [0.50000032]\n",
      " [0.49999945]\n",
      " [0.50000042]\n",
      " [0.49999929]\n",
      " [0.49999899]\n",
      " [0.50000034]\n",
      " [0.49999972]\n",
      " [0.49999889]\n",
      " [0.49999995]\n",
      " [0.50000158]\n",
      " [0.5000002 ]\n",
      " [0.49999965]\n",
      " [0.49999846]\n",
      " [0.4999999 ]\n",
      " [0.50000027]\n",
      " [0.49999976]\n",
      " [0.49999891]\n",
      " [0.49999971]\n",
      " [0.5000013 ]\n",
      " [0.4999999 ]\n",
      " [0.49999986]\n",
      " [0.50000016]\n",
      " [0.49999973]\n",
      " [0.49999955]\n",
      " [0.49999951]\n",
      " [0.50000039]\n",
      " [0.50000095]\n",
      " [0.50000014]\n",
      " [0.49999865]\n",
      " [0.50000053]\n",
      " [0.50000071]\n",
      " [0.50000079]\n",
      " [0.49999932]\n",
      " [0.50000043]\n",
      " [0.50000083]\n",
      " [0.49999997]\n",
      " [0.49999921]\n",
      " [0.49999896]\n",
      " [0.49999983]\n",
      " [0.50000002]\n",
      " [0.49999989]\n",
      " [0.5000003 ]\n",
      " [0.50000003]\n",
      " [0.49999918]\n",
      " [0.49999966]\n",
      " [0.49999921]\n",
      " [0.49999951]\n",
      " [0.50000186]\n",
      " [0.50000066]\n",
      " [0.50000035]\n",
      " [0.49999831]\n",
      " [0.5000002 ]\n",
      " [0.49999962]\n",
      " [0.5000004 ]\n",
      " [0.49999973]\n",
      " [0.50000029]\n",
      " [0.49999948]\n",
      " [0.49999974]\n",
      " [0.50000081]\n",
      " [0.49999941]\n",
      " [0.50000034]\n",
      " [0.49999989]\n",
      " [0.49999824]\n",
      " [0.49999887]\n",
      " [0.4999996 ]\n",
      " [0.50000004]\n",
      " [0.50000252]\n",
      " [0.50000033]\n",
      " [0.50000135]\n",
      " [0.49999977]\n",
      " [0.49999999]\n",
      " [0.50000063]\n",
      " [0.49999989]\n",
      " [0.5000004 ]\n",
      " [0.49999906]\n",
      " [0.49999983]\n",
      " [0.49999952]\n",
      " [0.49999976]\n",
      " [0.50000056]\n",
      " [0.49999864]\n",
      " [0.49999895]\n",
      " [0.50000153]\n",
      " [0.50000178]\n",
      " [0.50000045]\n",
      " [0.5000003 ]\n",
      " [0.50000074]\n",
      " [0.49999921]\n",
      " [0.49999996]\n",
      " [0.50000056]\n",
      " [0.49999894]\n",
      " [0.50000068]\n",
      " [0.50000086]\n",
      " [0.50000125]\n",
      " [0.49999903]\n",
      " [0.50000092]\n",
      " [0.50000058]\n",
      " [0.49999976]\n",
      " [0.50000007]\n",
      " [0.4999997 ]\n",
      " [0.49999935]\n",
      " [0.50000015]\n",
      " [0.50000011]\n",
      " [0.50000114]\n",
      " [0.49999953]\n",
      " [0.50000048]\n",
      " [0.49999975]\n",
      " [0.49999964]\n",
      " [0.4999989 ]\n",
      " [0.50000016]\n",
      " [0.5000004 ]\n",
      " [0.5000004 ]\n",
      " [0.50000023]\n",
      " [0.50000089]\n",
      " [0.49999991]\n",
      " [0.50000064]\n",
      " [0.49999967]\n",
      " [0.50000067]\n",
      " [0.49999993]\n",
      " [0.499999  ]\n",
      " [0.50000037]\n",
      " [0.49999964]\n",
      " [0.49999987]\n",
      " [0.49999974]\n",
      " [0.5000005 ]\n",
      " [0.50000111]\n",
      " [0.49999884]\n",
      " [0.50000046]\n",
      " [0.49999963]\n",
      " [0.49999893]\n",
      " [0.50000173]\n",
      " [0.50000042]\n",
      " [0.50000036]\n",
      " [0.50000148]\n",
      " [0.49999923]\n",
      " [0.50000031]\n",
      " [0.50000047]\n",
      " [0.49999972]\n",
      " [0.49999967]\n",
      " [0.49999905]\n",
      " [0.50000173]\n",
      " [0.5000002 ]\n",
      " [0.49999883]\n",
      " [0.49999941]\n",
      " [0.50000043]\n",
      " [0.49999934]\n",
      " [0.49999958]\n",
      " [0.49999987]\n",
      " [0.50000131]\n",
      " [0.49999927]\n",
      " [0.49999927]\n",
      " [0.50000007]\n",
      " [0.49999905]\n",
      " [0.49999946]\n",
      " [0.5000003 ]\n",
      " [0.50000149]\n",
      " [0.49999921]\n",
      " [0.4999999 ]\n",
      " [0.49999918]\n",
      " [0.49999861]\n",
      " [0.50000043]\n",
      " [0.49999975]\n",
      " [0.50000019]\n",
      " [0.50000114]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset from CSV\n",
    "df = pd.read_csv(\"student_dataset.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = df.drop(columns=[\"Final_Score\"]).values  # All columns except target\n",
    "y = df[\"Final_Score\"].values.reshape(-1, 1)  # Target column\n",
    "\n",
    "# Normalize features\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100  # Normalize target\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Weights initialization\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_derivative(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1)\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2)\n",
    "        self.output = self.sigmoid(self.z2)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        output_error = y - self.output  # Error in output\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output)  # Applying derivative of sigmoid\n",
    "        \n",
    "        hidden_error = output_delta.dot(self.W2.T)  # Error in hidden layer\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.a1)  # Applying derivative of sigmoid\n",
    "        \n",
    "        # Weight updates\n",
    "        self.W2 += self.a1.T.dot(output_delta) * self.learning_rate\n",
    "        self.W1 += X.T.dot(hidden_delta) * self.learning_rate\n",
    "    \n",
    "    def train(self, X, y, epochs=10000):\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean(np.square(y - self.output))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Initialize and train the neural network\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 8  # You can adjust this\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "NN = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "print(\"Initial Loss:\", np.mean(np.square(y - NN.forward(X))))\n",
    "\n",
    "NN.train(X, y)\n",
    "\n",
    "print(\"\\nFinal Predicted Output:\\n\", NN.forward(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a012c5f-451b-488a-a2e4-00df3c48e65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0257\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0168\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0099 \n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0051\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022 \n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3513e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2425e-04 \n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8654e-04 \n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5821e-04  \n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6661e-04 \n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6352e-04 \n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5791e-04 \n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6348e-04 \n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4571e-04 \n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4226e-04 \n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4411e-04 \n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3687e-04 \n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5169e-04 \n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3590e-04 \n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2852e-04 \n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3524e-04 \n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3816e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3583e-04 \n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1746e-04 \n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2656e-04 \n",
      "Epoch 26/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2047e-04 \n",
      "Epoch 27/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2575e-04 \n",
      "Epoch 28/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3816e-04 \n",
      "Epoch 29/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2656e-04 \n",
      "Epoch 30/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1505e-04 \n",
      "Epoch 31/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2237e-04 \n",
      "Epoch 32/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1698e-04 \n",
      "Epoch 33/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1602e-04 \n",
      "Epoch 34/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1536e-04 \n",
      "Epoch 35/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1061e-04 \n",
      "Epoch 36/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1351e-04 \n",
      "Epoch 37/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0643e-04 \n",
      "Epoch 38/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1388e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0069e-04 \n",
      "Epoch 40/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.7210e-05 \n",
      "Epoch 41/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1299e-04 \n",
      "Epoch 42/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0742e-04 \n",
      "Epoch 43/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.6356e-05 \n",
      "Epoch 44/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0363e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6005e-05 \n",
      "Epoch 46/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.1892e-05 \n",
      "Epoch 47/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.2254e-05 \n",
      "Epoch 48/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9243e-05 \n",
      "Epoch 49/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.8392e-05 \n",
      "Epoch 50/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.3504e-05 \n",
      "Epoch 51/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.7559e-05 \n",
      "Epoch 52/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.2667e-05 \n",
      "Epoch 53/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.4309e-05 \n",
      "Epoch 54/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1099e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1932e-05 \n",
      "Epoch 56/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.8802e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7542e-05 \n",
      "Epoch 58/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5431e-05 \n",
      "Epoch 59/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1419e-05 \n",
      "Epoch 60/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6085e-05 \n",
      "Epoch 61/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.0903e-05 \n",
      "Epoch 62/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.5119e-05 \n",
      "Epoch 63/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.3431e-05 \n",
      "Epoch 64/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5089e-05 \n",
      "Epoch 65/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.0188e-05 \n",
      "Epoch 66/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4902e-05 \n",
      "Epoch 67/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8773e-05 \n",
      "Epoch 68/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2986e-05 \n",
      "Epoch 69/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7593e-05 \n",
      "Epoch 70/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5493e-05 \n",
      "Epoch 71/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2313e-05 \n",
      "Epoch 72/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4377e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8918e-05 \n",
      "Epoch 74/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8411e-05 \n",
      "Epoch 75/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0987e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6754e-05 \n",
      "Epoch 77/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6215e-05 \n",
      "Epoch 78/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.3344e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3204e-05 \n",
      "Epoch 80/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2436e-05 \n",
      "Epoch 81/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5130e-05 \n",
      "Epoch 82/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0967e-05 \n",
      "Epoch 83/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3211e-05 \n",
      "Epoch 84/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2445e-05 \n",
      "Epoch 85/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5143e-05 \n",
      "Epoch 86/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9198e-05 \n",
      "Epoch 87/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6998e-05 \n",
      "Epoch 88/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8261e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.7191e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5840e-05 \n",
      "Epoch 91/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7260e-05 \n",
      "Epoch 92/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1711e-05 \n",
      "Epoch 93/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0964e-05 \n",
      "Epoch 94/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4124e-05 \n",
      "Epoch 95/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0600e-05 \n",
      "Epoch 96/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0908e-05 \n",
      "Epoch 97/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.1885e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8796e-05 \n",
      "Epoch 99/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4199e-05 \n",
      "Epoch 100/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0911e-05 \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "Final Predicted Output:\n",
      " [[0.5043042 ]\n",
      " [0.50950265]\n",
      " [0.4929512 ]\n",
      " [0.4984151 ]\n",
      " [0.49091393]\n",
      " [0.49078745]\n",
      " [0.49884862]\n",
      " [0.50508857]\n",
      " [0.50891626]\n",
      " [0.49897805]\n",
      " [0.4884223 ]\n",
      " [0.505919  ]\n",
      " [0.5034634 ]\n",
      " [0.4944626 ]\n",
      " [0.49689326]\n",
      " [0.4924137 ]\n",
      " [0.5022292 ]\n",
      " [0.49545795]\n",
      " [0.50112   ]\n",
      " [0.4995613 ]\n",
      " [0.5005061 ]\n",
      " [0.507669  ]\n",
      " [0.49342608]\n",
      " [0.4978627 ]\n",
      " [0.49969578]\n",
      " [0.50210285]\n",
      " [0.4944002 ]\n",
      " [0.49382597]\n",
      " [0.5083153 ]\n",
      " [0.49462757]\n",
      " [0.50546503]\n",
      " [0.4994748 ]\n",
      " [0.49052346]\n",
      " [0.5101948 ]\n",
      " [0.5133067 ]\n",
      " [0.50619453]\n",
      " [0.50834405]\n",
      " [0.4905692 ]\n",
      " [0.5020112 ]\n",
      " [0.50841033]\n",
      " [0.49855053]\n",
      " [0.48751363]\n",
      " [0.49577653]\n",
      " [0.50707567]\n",
      " [0.4929972 ]\n",
      " [0.5001583 ]\n",
      " [0.49951684]\n",
      " [0.4993651 ]\n",
      " [0.4959235 ]\n",
      " [0.49728385]\n",
      " [0.5014816 ]\n",
      " [0.50861716]\n",
      " [0.5080963 ]\n",
      " [0.5148901 ]\n",
      " [0.49926   ]\n",
      " [0.5079866 ]\n",
      " [0.49601597]\n",
      " [0.49263936]\n",
      " [0.49808878]\n",
      " [0.49804044]\n",
      " [0.49886316]\n",
      " [0.49364105]\n",
      " [0.49876052]\n",
      " [0.50406235]\n",
      " [0.49765617]\n",
      " [0.4945432 ]\n",
      " [0.49310964]\n",
      " [0.5091568 ]\n",
      " [0.49746755]\n",
      " [0.50750446]\n",
      " [0.50343937]\n",
      " [0.5002552 ]\n",
      " [0.50397086]\n",
      " [0.5111809 ]\n",
      " [0.49348605]\n",
      " [0.50509393]\n",
      " [0.49418548]\n",
      " [0.49657667]\n",
      " [0.504073  ]\n",
      " [0.48922753]\n",
      " [0.4989394 ]\n",
      " [0.5034158 ]\n",
      " [0.49972132]\n",
      " [0.49305856]\n",
      " [0.49997565]\n",
      " [0.49494192]\n",
      " [0.5042956 ]\n",
      " [0.49930826]\n",
      " [0.50751746]\n",
      " [0.50168526]\n",
      " [0.49082264]\n",
      " [0.4950376 ]\n",
      " [0.49815923]\n",
      " [0.4966916 ]\n",
      " [0.511904  ]\n",
      " [0.4986757 ]\n",
      " [0.50478023]\n",
      " [0.50258756]\n",
      " [0.49570113]\n",
      " [0.49429035]\n",
      " [0.49406737]\n",
      " [0.50125897]\n",
      " [0.4917012 ]\n",
      " [0.50849193]\n",
      " [0.49889714]\n",
      " [0.496472  ]\n",
      " [0.49311456]\n",
      " [0.49974927]\n",
      " [0.4928925 ]\n",
      " [0.49796283]\n",
      " [0.49296474]\n",
      " [0.50205904]\n",
      " [0.51297253]\n",
      " [0.49970022]\n",
      " [0.49713227]\n",
      " [0.5079974 ]\n",
      " [0.50192976]\n",
      " [0.4913805 ]\n",
      " [0.51037884]\n",
      " [0.49754286]\n",
      " [0.49857408]\n",
      " [0.49034786]\n",
      " [0.4926937 ]\n",
      " [0.50061977]\n",
      " [0.49363005]\n",
      " [0.4965392 ]\n",
      " [0.50021744]\n",
      " [0.5105531 ]\n",
      " [0.4972421 ]\n",
      " [0.4921739 ]\n",
      " [0.49571183]\n",
      " [0.4961649 ]\n",
      " [0.50022703]\n",
      " [0.49788427]\n",
      " [0.51458025]\n",
      " [0.5052545 ]\n",
      " [0.5069193 ]\n",
      " [0.50043666]\n",
      " [0.49509394]\n",
      " [0.5029594 ]\n",
      " [0.5012903 ]\n",
      " [0.5040742 ]\n",
      " [0.50441206]\n",
      " [0.49321592]\n",
      " [0.5003395 ]\n",
      " [0.5037188 ]\n",
      " [0.4939308 ]\n",
      " [0.49733627]\n",
      " [0.4948139 ]\n",
      " [0.4945516 ]\n",
      " [0.49831718]\n",
      " [0.49929127]\n",
      " [0.49472442]\n",
      " [0.49669045]\n",
      " [0.50437105]\n",
      " [0.49559805]\n",
      " [0.5038278 ]\n",
      " [0.49725682]\n",
      " [0.49588767]\n",
      " [0.5053017 ]\n",
      " [0.4862255 ]\n",
      " [0.5040693 ]\n",
      " [0.50249237]\n",
      " [0.50362563]\n",
      " [0.4972754 ]\n",
      " [0.49486327]\n",
      " [0.49800625]\n",
      " [0.49652302]\n",
      " [0.49666783]\n",
      " [0.5047363 ]\n",
      " [0.4996866 ]\n",
      " [0.49448246]\n",
      " [0.50066805]\n",
      " [0.4980306 ]\n",
      " [0.506497  ]\n",
      " [0.489083  ]\n",
      " [0.501273  ]\n",
      " [0.49883953]\n",
      " [0.50573623]\n",
      " [0.49762368]\n",
      " [0.50247777]\n",
      " [0.4882923 ]\n",
      " [0.49919605]\n",
      " [0.501376  ]\n",
      " [0.49977505]\n",
      " [0.50252205]\n",
      " [0.50114053]\n",
      " [0.50296235]\n",
      " [0.49869204]\n",
      " [0.48849007]\n",
      " [0.50393057]\n",
      " [0.5097122 ]\n",
      " [0.4975373 ]\n",
      " [0.49561667]\n",
      " [0.49107134]\n",
      " [0.49097317]\n",
      " [0.49368575]\n",
      " [0.5020228 ]\n",
      " [0.49865982]\n",
      " [0.50117844]\n",
      " [0.4992367 ]\n",
      " [0.50109994]\n",
      " [0.504583  ]\n",
      " [0.49944496]\n",
      " [0.5021522 ]\n",
      " [0.49467   ]\n",
      " [0.50133544]\n",
      " [0.5004989 ]\n",
      " [0.4938441 ]\n",
      " [0.496215  ]\n",
      " [0.50280094]\n",
      " [0.50271386]\n",
      " [0.5040926 ]\n",
      " [0.49165687]\n",
      " [0.49403372]\n",
      " [0.5026592 ]\n",
      " [0.5004245 ]\n",
      " [0.49712333]\n",
      " [0.50608826]\n",
      " [0.51024693]\n",
      " [0.5116899 ]\n",
      " [0.4978825 ]\n",
      " [0.48267874]\n",
      " [0.4988006 ]\n",
      " [0.49602422]\n",
      " [0.49915373]\n",
      " [0.50309926]\n",
      " [0.5042994 ]\n",
      " [0.5042753 ]\n",
      " [0.4992431 ]\n",
      " [0.50041354]\n",
      " [0.50447   ]\n",
      " [0.5055257 ]\n",
      " [0.5002762 ]\n",
      " [0.49793884]\n",
      " [0.5152228 ]\n",
      " [0.48616725]\n",
      " [0.49568912]\n",
      " [0.50048006]\n",
      " [0.50255775]\n",
      " [0.49767885]\n",
      " [0.5020678 ]\n",
      " [0.50627315]\n",
      " [0.4868067 ]\n",
      " [0.49698195]\n",
      " [0.50636446]\n",
      " [0.4982409 ]\n",
      " [0.50647914]\n",
      " [0.50347555]\n",
      " [0.50022995]\n",
      " [0.49312252]\n",
      " [0.49825296]\n",
      " [0.5037261 ]\n",
      " [0.49413547]\n",
      " [0.48622918]\n",
      " [0.501985  ]\n",
      " [0.5067507 ]\n",
      " [0.49249128]\n",
      " [0.50502104]\n",
      " [0.49427328]\n",
      " [0.48948726]\n",
      " [0.5082732 ]\n",
      " [0.49382102]\n",
      " [0.49762085]\n",
      " [0.503739  ]\n",
      " [0.5043998 ]\n",
      " [0.51174974]\n",
      " [0.512181  ]\n",
      " [0.4898956 ]\n",
      " [0.49615243]\n",
      " [0.50384176]\n",
      " [0.5061378 ]\n",
      " [0.5108037 ]\n",
      " [0.51254237]\n",
      " [0.50500643]\n",
      " [0.50176847]\n",
      " [0.50335693]\n",
      " [0.5019071 ]\n",
      " [0.505383  ]\n",
      " [0.4979519 ]\n",
      " [0.50721323]\n",
      " [0.49026564]\n",
      " [0.4960568 ]\n",
      " [0.5026307 ]\n",
      " [0.49443188]\n",
      " [0.50418806]\n",
      " [0.5052518 ]\n",
      " [0.50867194]\n",
      " [0.49452388]\n",
      " [0.50567716]\n",
      " [0.4917542 ]\n",
      " [0.496083  ]\n",
      " [0.5067592 ]\n",
      " [0.49697408]\n",
      " [0.49168298]\n",
      " [0.50334734]\n",
      " [0.50493383]\n",
      " [0.49559978]\n",
      " [0.5033242 ]\n",
      " [0.493344  ]\n",
      " [0.5077114 ]\n",
      " [0.49754733]\n",
      " [0.50981665]\n",
      " [0.5059335 ]\n",
      " [0.508188  ]\n",
      " [0.5008184 ]\n",
      " [0.50784373]\n",
      " [0.4948402 ]\n",
      " [0.5136381 ]\n",
      " [0.50771105]\n",
      " [0.50381577]\n",
      " [0.48954442]\n",
      " [0.49989772]\n",
      " [0.50263625]\n",
      " [0.50639474]\n",
      " [0.50026166]\n",
      " [0.49117002]\n",
      " [0.5017736 ]\n",
      " [0.49000105]\n",
      " [0.50524735]\n",
      " [0.5050231 ]\n",
      " [0.49859542]\n",
      " [0.49852717]\n",
      " [0.50076723]\n",
      " [0.5024909 ]\n",
      " [0.50146675]\n",
      " [0.5008563 ]\n",
      " [0.49972543]\n",
      " [0.5077356 ]\n",
      " [0.4977853 ]\n",
      " [0.5021494 ]\n",
      " [0.4999753 ]\n",
      " [0.49765015]\n",
      " [0.4961675 ]\n",
      " [0.49462533]\n",
      " [0.48707324]\n",
      " [0.5035875 ]\n",
      " [0.4986105 ]\n",
      " [0.51169825]\n",
      " [0.49411768]\n",
      " [0.49877167]\n",
      " [0.5025356 ]\n",
      " [0.48961517]\n",
      " [0.5065235 ]\n",
      " [0.49372208]\n",
      " [0.50272   ]\n",
      " [0.4956897 ]\n",
      " [0.49054173]\n",
      " [0.49396718]\n",
      " [0.50360256]\n",
      " [0.4917179 ]\n",
      " [0.5058429 ]\n",
      " [0.50535196]\n",
      " [0.4989783 ]\n",
      " [0.49073324]\n",
      " [0.500538  ]\n",
      " [0.4975229 ]\n",
      " [0.4972782 ]\n",
      " [0.5133538 ]\n",
      " [0.49570757]\n",
      " [0.49711114]\n",
      " [0.5038573 ]\n",
      " [0.4958628 ]\n",
      " [0.48545134]\n",
      " [0.5028154 ]\n",
      " [0.5054281 ]\n",
      " [0.51222533]\n",
      " [0.49663848]\n",
      " [0.4925905 ]\n",
      " [0.50630903]\n",
      " [0.49222147]\n",
      " [0.49392605]\n",
      " [0.49805728]\n",
      " [0.5111222 ]\n",
      " [0.4994245 ]\n",
      " [0.49996105]\n",
      " [0.50594383]\n",
      " [0.49662268]\n",
      " [0.49949574]\n",
      " [0.48981866]\n",
      " [0.49186662]\n",
      " [0.5007278 ]\n",
      " [0.5019911 ]\n",
      " [0.5082993 ]\n",
      " [0.4978364 ]\n",
      " [0.49887073]\n",
      " [0.49820945]\n",
      " [0.4997684 ]\n",
      " [0.49991077]\n",
      " [0.50024885]\n",
      " [0.51316047]\n",
      " [0.500907  ]\n",
      " [0.5017596 ]\n",
      " [0.501655  ]\n",
      " [0.50835145]\n",
      " [0.5046412 ]\n",
      " [0.50186807]\n",
      " [0.49906874]\n",
      " [0.50576776]\n",
      " [0.5034503 ]\n",
      " [0.49310708]\n",
      " [0.4996512 ]\n",
      " [0.50189143]\n",
      " [0.50363314]\n",
      " [0.4909392 ]\n",
      " [0.4972812 ]\n",
      " [0.5050281 ]\n",
      " [0.49473262]\n",
      " [0.50702024]\n",
      " [0.49516395]\n",
      " [0.49956685]\n",
      " [0.5019126 ]\n",
      " [0.5118387 ]\n",
      " [0.5079669 ]\n",
      " [0.5075986 ]\n",
      " [0.49624187]\n",
      " [0.49794745]\n",
      " [0.5067246 ]\n",
      " [0.4973492 ]\n",
      " [0.49407864]\n",
      " [0.5058672 ]\n",
      " [0.51247776]\n",
      " [0.50037587]\n",
      " [0.5047147 ]\n",
      " [0.4940555 ]\n",
      " [0.49986038]\n",
      " [0.5022463 ]\n",
      " [0.5118052 ]\n",
      " [0.49730405]\n",
      " [0.48703083]\n",
      " [0.48822826]\n",
      " [0.50337183]\n",
      " [0.51379037]\n",
      " [0.49081713]\n",
      " [0.4899326 ]\n",
      " [0.50720036]\n",
      " [0.4855954 ]\n",
      " [0.5057297 ]\n",
      " [0.50354487]\n",
      " [0.5034219 ]\n",
      " [0.49219385]\n",
      " [0.5081495 ]\n",
      " [0.49973476]\n",
      " [0.5023517 ]\n",
      " [0.5070896 ]\n",
      " [0.5033523 ]\n",
      " [0.49422315]\n",
      " [0.50185   ]\n",
      " [0.49749205]\n",
      " [0.5002126 ]\n",
      " [0.5049154 ]\n",
      " [0.4889438 ]\n",
      " [0.49424863]\n",
      " [0.5078124 ]\n",
      " [0.4946983 ]\n",
      " [0.495158  ]\n",
      " [0.49728692]\n",
      " [0.49154082]\n",
      " [0.49337563]\n",
      " [0.4982237 ]\n",
      " [0.4855997 ]\n",
      " [0.5020466 ]\n",
      " [0.50216913]\n",
      " [0.5007451 ]\n",
      " [0.5014581 ]\n",
      " [0.5029788 ]\n",
      " [0.4895709 ]\n",
      " [0.5057404 ]\n",
      " [0.4948921 ]\n",
      " [0.50327206]\n",
      " [0.49377304]\n",
      " [0.5008242 ]\n",
      " [0.4934984 ]\n",
      " [0.5045201 ]\n",
      " [0.49704117]\n",
      " [0.5067475 ]\n",
      " [0.4953515 ]\n",
      " [0.497376  ]\n",
      " [0.5096199 ]\n",
      " [0.4974187 ]\n",
      " [0.49696743]\n",
      " [0.5030044 ]\n",
      " [0.49593008]\n",
      " [0.5070638 ]\n",
      " [0.49601978]\n",
      " [0.5013292 ]\n",
      " [0.49243897]\n",
      " [0.4985572 ]\n",
      " [0.50605947]\n",
      " [0.5082358 ]\n",
      " [0.50663507]\n",
      " [0.49933317]\n",
      " [0.49678904]\n",
      " [0.5067052 ]\n",
      " [0.4956265 ]\n",
      " [0.49605504]\n",
      " [0.49899265]\n",
      " [0.49005577]\n",
      " [0.5104964 ]\n",
      " [0.51071507]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset from CSV\n",
    "df = pd.read_csv(\"student_dataset.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = df.drop(columns=[\"Final_Score\"]).values  # All columns except target\n",
    "y = df[\"Final_Score\"].values.reshape(-1, 1)  # Target column\n",
    "\n",
    "# Normalize features\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100  # Normalize target\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(8, activation='sigmoid', input_shape=(X.shape[1],)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predicted_output = model.predict(X)\n",
    "print(\"\\nFinal Predicted Output:\\n\", predicted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba675ee-66dc-48b5-a72a-82f1116dcac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m load_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudent_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#if not data: \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#    raise ValueError(\"Dataset is empty or improperly formatted.\")\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[0;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m [row[:input_size] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data] \n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m [[row[input_size] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data]  \u001b[38;5;66;03m# Normalize target\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#Load dataset manually without libraries\n",
    "\n",
    "def load_csv(filename): \n",
    "    data = [] \n",
    "    with open(filename, 'r') as file: \n",
    "        for line in file: \n",
    "            row = line.strip().split(',') \n",
    "            if len(row) > 1:  # Ensure non-empty row \n",
    "                try: \n",
    "                    data.append([float(value) for value in row]) \n",
    "                except ValueError: \n",
    "                    continue  # Skip rows with invalid data return data[1:]  # Skip header\n",
    "data = load_csv(\"student_dataset.csv\")\n",
    "#if not data: \n",
    "#    raise ValueError(\"Dataset is empty or improperly formatted.\")\n",
    "\n",
    "input_size = len(data[0]) - 1 \n",
    "X = [row[:input_size] for row in data] \n",
    "y = [[row[input_size] / 100] for row in data]  # Normalize target\n",
    "\n",
    "#Initialize weights\n",
    "\n",
    "def initialize_weights(input_size, hidden_size, output_size): \n",
    "    W1 = [[(i + j) * 0.01 for j in range(hidden_size)] for i in range(input_size)] \n",
    "    W2 = [[(i + j) * 0.01 for j in range(output_size)] for i in range(hidden_size)] \n",
    "    return W1, W2\n",
    "\n",
    "#Activation function\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + (2.71828 ** -x))  # Approximate e\n",
    "\n",
    "def sigmoid_derivative(x): \n",
    "    return x * (1 - x)\n",
    "\n",
    "#Forward pass\n",
    "\n",
    "def forward_pass(X, W1, W2): \n",
    "    z = [[sum(i * w for i, w in zip(row, col)) for col in zip(*W1)] for row in X] \n",
    "    z2 = [[sigmoid(val) for val in row] for row in z] \n",
    "    z3 = [[sum(i * w for i, w in zip(row, col)) for col in zip(*W2)] for row in z2] \n",
    "    o = [[sigmoid(val) for val in row] for row in z3] \n",
    "    return z, z2, z3, o\n",
    "\n",
    "#Backpropagation\n",
    "\n",
    "def backward_pass(X, y, W1, W2, z, z2, z3, o, learning_rate=0.01): \n",
    "    o_error = [[yi - oi for yi, oi in zip(y_row, o_row)] for y_row, o_row in zip(y, o)] \n",
    "    o_delta = [[err * sigmoid_derivative(oi) for err, oi in zip(err_row, o_row)] for err_row, o_row in zip(o_error, o)]\n",
    "\n",
    "    z2_error = [[sum(d * w for d, w in zip(delta_row, col)) for col in zip(*W2)] for delta_row in o_delta]\n",
    "    z2_delta = [[err * sigmoid_derivative(zi) for err, zi in zip(err_row, z2_row)] for err_row, z2_row in zip(z2_error, z2)]\n",
    "\n",
    "    for i in range(len(W1)):\n",
    "        for j in range(len(W1[i])):\n",
    "            W1[i][j] += learning_rate * sum(X[k][i] * z2_delta[k][j] for k in range(len(X)))\n",
    "\n",
    "    for i in range(len(W2)):\n",
    "        for j in range(len(W2[i])):\n",
    "            W2[i][j] += learning_rate * sum(z2[k][i] * o_delta[k][j] for k in range(len(z2)))\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "#Training function\n",
    "\n",
    "def train(X, y, hidden_size=8, epochs=100, learning_rate=0.01): \n",
    "    W1, W2 = initialize_weights(len(X[0]), hidden_size, 1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        z, z2, z3, o = forward_pass(X, W1, W2)\n",
    "        W1, W2 = backward_pass(X, y, W1, W2, z, z2, z3, o, learning_rate)\n",
    "\n",
    "    return W1, W2, o\n",
    "\n",
    "#rain the model\n",
    "\n",
    "W1, W2, predictions = train(X, y)\n",
    "\n",
    "#Print results\n",
    "\n",
    "print(\"\\nInput: \\n\", X) \n",
    "print(\"\\nActual Output: \\n\", y)\n",
    "print(\"\\nPredicted Output: \\n\", predictions) \n",
    "print(\"\\nLoss: \\n\", sum(sum((yi - oi) ** 2 for yi, oi in zip(y_row, o_row)) for y_row, o_row in zip(y, predictions)) / len(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
